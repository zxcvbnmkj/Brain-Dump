# 《生成式人工智能导论》观后感

## 提示词（最有性价比，不改变模型参数的情况下提升其性能）
把大模型想象成一个线上的新人助理，它具备处理问题的能力，只是不知道问题背景和流程，这才是我们应当在提示词中清楚说明的内容

![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251009022844446-paste.png)

### 可能提高模型效果的提示词设计方法
- `CoT` : 对于复杂任务，提示词可以将任务拆解成小的步骤，引导大模型思考。（简单任务应该没有必要）
- 强化学习 + 自动化测试 : 尝试不同的提示词，筛选出较好的。

得到的好提示词居然是人类不理解的乱七八糟的提示词，比如：ways ways ways ....
- 提供示例（ In-context Learning 技术）
- `RAG`
- `PoT` ： （Program of Thoughts）: 类似于智能体，调用外部工具。比如在解复杂数学问题，会先生成代码，然后调用外部工具执行代码。（生成特殊符号，用来表示需要调用的工具编号）
- `ToT` : 生成-反省-选择

![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251009105731860-paste.png)

- 多模型合作 ：类似于多智能体系统。比如：上游模型选择下游模型、反省、讨论

通过判断是否达成共识来决定是否停止讨论。可以设置一个裁判模型判断

![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251009092100224-paste.png)


### 关于提示词的有趣现象（来源于一些论文对奇奇怪怪都市传说的测试，仅对部分模型有效）
- 情绪勒索似乎可以提升大模型输出内容的正确性，如提示词中强调 “这件事对我非常重要”
- 让大模型解释一下自己的回答，有助于提升回答准确率
- 对大模型有礼貌并不会得到性能提升
- 不要对大模型说反话，避免输入：不要干什么、禁止干什么，直接输入需要做什么
- 口头威胁或假装奖励居然能提升大模型效果
- 尝试问大模型，我需要解决一个什么问题，有没有什么准确率比较好的提示词
- 让大模型深呼吸一下，它的能力变得更强了😂
- `CoT` 完成后，多加一个步骤，让大模型检查自己的输出，判断是否有误，可提升效果

### 疑问
在冻结模型参数并保证提示词一样的情况下，即使设置了 `temperature=0` 和 `top_k=1` ，理论上每一次运行所得输出应该是一样的，但实际上仍然可能出现不同的结果

原因可能是：
- 不确定性主要归咎于计算机处理浮点数时顺序不同会导致微小误差，而并行运算又放大了这些差异
- 批次不变性缺失，服务器一次处理多少请求，会影响最终输出

保证生成结果的一致性的好处：
- 有助于提升模型可接受性
- 对需保证可靠性的任务来说有重要意义
- 在强化学习中可以更严格的控制变量

目前有论文[[1]](https://arxiv.org/abs/2506.09501)对这一方面进行研究[[2]](https://it.sohu.com/a/933907971_211762)

### 模型构建过程
预训练阶段：所选择的语料质量非常重要，若在混乱的语料上预训练，模型参数规模再大性能也难以提升


### 模型可解释性
- `local explain`
- `global explain`
