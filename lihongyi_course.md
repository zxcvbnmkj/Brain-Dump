# 《生成式人工智能导论》分享
结构是：
- 课程要点总结：回顾
- 话题：从视频内容中引申出的知识点或者值得讨论的东西

有错误和不足请大家指正和补充
## 第一讲：生成式 AI 是什么？
### 课程要点总结
如何做到不断生成？
- 文字接龙模式，不断把本次生成的一个字追加到输入序列末尾，这种方法称为**自回归生成（Autoregressive Generation）**
- **生成问题的本质是分类问题**，类别个数 = 词汇表中的 token 个数，模型输出每个 token 的概率
- 理论上，对于图片生成 AI，可以使用像素接龙，openAI 曾经有尝试过这种方法，但这种方法有缺陷，现在已经不采用，已被扩散模型代替
### 话题
- 词汇表起到 word2id 的作用，BERT 的词汇表非常好找，是 `vocab.txt`，大模型的则一般是 `tokenizer.json`
- 一个分析输入序列中有哪些 token （GPT 中的）的网站

![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030105053817-paste.png)
- Tokenizer -> Embedding -> 位置编码 -> Attention ->
- 同一个 Token 的词向量一定是一样的，这一步不考虑多义词。经过注意力之后模型可以结合上下文判断多义词在上下文中的含义。注意力聚合上下文的词向量到本词的词向量中，聚合权重是计算本词与上下文词两两之间的相关性
![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030110418063-paste.png)
- 大模型只计算前文上下文词间的相关性
- 在每一个Transformer层中有多头注意力：从不同角度判断相关性（权重），不同组的输出通过前馈网络使得维度复原
![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030110617760-paste.png)
![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030110829675-paste.png)
- 堆叠多个 Transformer 层，把最后一层的最后一个 token 向量做归一化，得到下一个词的概率分别
![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030111014971-paste.png)

- 位置编码是把位置也转换为向量，转换过程可以是人工设计，也可是自动学习，位置编码将于词向量拼接

![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030105733870-paste.png)
- 研究方向
![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030111631608-paste.png)

- Transformer 采用**编码器-解码器**结构，几乎所有的大模型都基于 Transformer ，但是目前主流的大模型（GPT、LLaMA 等）结构却是**仅解码器（Dncoder-only）** 并使用**单向注意力**（只关注前文词语），因为这样更加高效。
- BERT 以及它的变体（如 RoBERTa）则采用**仅编码器（Encoder-only）** 结构并使用**双向注意力**。因为 BERT 的核心能力是**语义理解**，它一般用于分类任务。
## 第三/四/五讲：山不过来，我就过去
### 课程要点总结
- 把大模型想象成一个刚来的实习生，它具备处理问题的基本能力，只是不知道问题背景和处理流程，因此提示词工程可以解决大模型的大部分问题。
#### 可能提高模型效果的提示词设计方法
- **强化学习 + 自动化测试** : 尝试不同的提示词，筛选出较好的，提示词由另一个模型生成。但是得到的可能是无厘头提示词，比如某任务得到的得到的最佳提示词居然是`ways ways ways ....`
- **In-context Learning 技术**：即提供一个示例
- **RAG**
- **思维链 CoT** : 对于复杂任务，提示词可以将任务拆解成小的步骤，引导大模型思考。（简单任务应该没有必要）
- **程序思维 PoT（Program of Thoughts）**: 调用外部工具。比如，在解复杂数学问题，会先生成代码，然后调用外部工具执行代码。（生成特殊符号，用来表示需要调用的工具编号）
- **思维树 ToT**：生成-反省-选择

![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251009105731860-paste.png)

- **多模型合作** ：类似于多智能体系统。比如：上游模型选择下游模型、反省、讨论

通过判断是否达成共识来决定是否停止讨论。可以设置一个裁判模型判断

![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251009092100224-paste.png)
#### 关于提示词的有趣现象（来源于一些论文对奇奇怪怪都市传说的测试，仅对部分模型有效）
1. 应当遵循
- 不要对大模型说反话，避免输入：不要干什么、禁止干什么，直接输入需要做什么
- 对大模型有礼貌并不会得到性能提升（不需要加“请”）
2. 技巧
- 情绪勒索似乎可以提升大模型输出内容的正确性，如提示词中强调 “这件事对我非常重要”
- 让大模型解释一下自己的回答，有助于提升回答准确率
- 口头威胁或假装奖励居然能提升大模型效果
- 尝试问大模型，我需要解决一个什么问题，有没有什么准确率比较好的提示词
- 让大模型深呼吸一下，它的能力变得更强了😂
- `CoT` 完成后，多加一个步骤，让大模型检查自己的输出，判断是否有误，可提升效果
### 话题
#### 追根溯源- - - 最初的思维链到底是什么样子的？
思维链在现在来看是一个老概念，也衍生出了非常多变体，但它最初被提出时，是怎么写的？

先说我认为的思维链写法：
1. 像 Action 部分一样，告诉大模型第一步干什么、第二步干什么；
2. 上面的写法没有泛用性，要人工想步骤。也许还可以写成：
- 从已知信息出发，逐步推导未知；
- 遇到复杂问题先拆分模块，再逐个解决；

思维链是谷歌在 2022 年在论文[《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》](https://arxiv.org/abs/2201.11903v5)中提出，里面似乎是用举例的方法（In-context Learning）来做的，下图 1 是论文附图，图 2 是附录 G 中展示的谷歌使用的完整提示词。

![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251029153124135-paste.png)

![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251029154455831-paste.png)
论文中提到有<输入、思维链、输出>，但它的思维链部分似乎不在输入中，而是在输出中 ？？？
> 我们探索了语言模型在给定由三元组组成的提示的情况下对推理任务执行少量提示的能力：
⟨
 输入、 思维链 、输出
⟩
 。 思维链是导致最终输出的一系列中间自然语言推理步骤，我们将这种方法称为思维链提示。对算术、常识和符号推理基准进行了实证评估，表明思维链提示优于标准提示，有时甚至达到惊人的程度。

![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251029154858581-paste.png)

#### 豆包、deepseek 等大模型都有“深度思考”功能，它和思维链有关吗？
在选中了大模型的“深度思考”按钮之后会发生什么？

思考->初步想法->反省->发现漏洞→修正思路→进行下一步思考

这是思维链的应用
#### 思维链是很旧的技术吗？提示词工程中用的很少
- 一个技术很旧不代表它完全没用，就算是机器学习、LSTM 这种在特定任务中也有独特优势。感觉是看这个技术现在是不是研究人员的第一选择，有没有出现它的上位替代品。
- 智能体算是上位吗？智能体中的“规划”功能会不会和思维链重合。但感觉是智能体是思维链的应用场景之一，是智能体内部会用到思维链。
- 思维链、workflow、提示词中的 action 部分间的关系？
## 第六/七/八讲：大模型修炼史
### 课程要点总结
#### 第一阶段：预训练
即在海量语料上做预测下一个字是什么的自监督学习

预训练阶段所选择的语料质量非常重要，若在混乱的语料上预训练，模型参数规模再大性能也难以提升
#### 第二阶段：微调
构建问答对，有监督的训练模型
#### 第三阶段：强化学习
- 要人类去打标签会比较困难，但是去判断哪个答案比较好，会更轻松
- 要有效的利用人类回馈
- 除了生成式语言模型之后，还要一个**回馈模型**，该模型的输入是：“问题1、回答1”；“问题1：回答2...”，针对每一个问答对，回馈模型会输出一个分数，这个分数可代表人类的喜好。若得到一个低分，语言模型会微调参数，降低这个输出出现的概率
- 回馈模型和训练中的语言模型可以是同一个模型，利用大模型的反省能力
![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251029170407939-paste.png)
### 话题
#### 模型可解释性（该课程中的扩展一讲有谈到）
在冻结模型参数并保证提示词一样的情况下，即使设置了 `temperature=0.0` 和 `top_k=1` ，理论上每一次运行所得输出应该是一样的，但实际上大模型仍然可能输出不同的结果。

琪琪哥之前谈到拿大模型做分类任务的方法：固定前几个词的，看分歧字（“已”或者“未”）的概率

有两个没听明白的地方：

疑问1：为什么提示词中不让大模型直接只输出一个字，比如“是”或者“否”，这样就避免了生成前几个固定字

疑问2：获取概率是为了自己来指定阈值吗，因为从大模型生成的回答可以直接看出它的分类结果

##### 为什么大模型每次输出概率不同
- 电脑处理浮点数时顺序不同会导致微小误差，而并行运算又放大了这些差异
- 批次导致，服务器一次处理多少请求，会影响最终输出

##### 如果生成结果的一致的好处
- 提升模型可接受性
- 对需保证可靠性的任务来说有用
- 在强化学习中可以更严格的控制变量

##### 相关研究：[《Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference》](https://arxiv.org/abs/2506.09501)
- 在**贪婪解码（即每次生成时都选择概率最高的词）** 和 `bfloat16` 精度下，由于 **GPU 数量、类型和评估批量大小**的差异，DeepSeek-R1-Distill、Qwen-7B推理模型可能会表现出高达 9% 的准确率变化和 9,000 个标记的响应长度差异。
- 贪婪解码使得在令牌生成过程中会积累微小的数值误差，最终导致不同运行的输出存在显着差异
- FP32 精度比起 FP16 更能提高结果的重现性
- 提出一种优化的 FP32 推理管道，它在 FP32 中执行所有计算，并以 BF16 精度保留模型权重，从而有效地平衡了内存效率和可重复性，并把它作为 vLLM 的补丁发布，只需更改几行代码即可使用
- 结果（还经过一系列操作所得）：选用了 2350 亿参数的 Qwen3-235B-A22B-Instruct-2507 模型进行实验。经过 1000 次重复测试，该模型在相同输入条件下实现了 100% 的输出一致性[[1]](https://it.sohu.com/a/933907971_211762)

## 第九讲：智能体
- 多步骤的复杂任务中，希望让 AI 可以自己做规划，并工具实际情况调整规划
- 可以体验的已有智能体
![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030103755949-paste.png)
- 智能体的结构
![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030104124372-paste.png)
- 大模型在开启新一轮对话之后，之前的记忆就会去除，但是智能体具有长短期记忆能力，可以存储过去的关键信息
### 智能体的理论定义
### 怎么实现，是通过低代码工具吗？
## 第十讲：现下的语言模型是怎么做文字接龙的
## 评估大模型的性能
- 选择题数据集
- 翻译任务：BLEU 指标
- 摘要任务：ROUGE 指标
- [语言模型竞技场](https://chat.lmsys.org/) + 得分榜 ：人工对比两个模型的输出
- 提供标准答案之后，让大模型来判断代替人工
- 多种任务上判断性能，[BIG-bench](2206.04615)中收集了非常多各种各样的任务
- 验证大模型的长文本阅读能力，大海捞针方法，在一堆文档中插入一段关键信息，问大模型关于该段信息的内容
## 大模型安全
### 幻觉
- 大模型幻觉问题只能缓解不能传递解决（至少在今天看来）
- 事实查核：检查大模型输出内容是否是真实的，有 Factscore、FacTool 两种，它们是从生成文本中提取出需要验证的内容，再去网络上检索相关知识，再对比生成内容和搜索到的内容。
  - 网上的东西也参差不齐
  - 怎么判断哪些需要检索的，可以不准
###
- 替换输入内容中的，性别、种族、地域等信息之后再让大模型回答

![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030114833503-paste.png)
- 训练另一个模型去刺激大模型，训练过程中该模型学习差距最大化的情况
![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030115210938-paste.png)
- 彭博社实验，让大模型去筛选简历，让它对简历排序，重复一千次，发现确实是会有偏好，而且可能会和人类社会的刻板印象不一样，比如某个大模型认为白人女性最适合从事软件开发工作
- 把大量姓名词嵌入后再降到 2 维，得到的散点图存在不同种族名字聚集情况，可能是偏见来源
- 大模型的刻板印象，让它给幼儿园老师写一封信，大模型默认使用 Miss，如果是建筑工程，则默认是男性
![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030115924058-paste.png)
- 大模型的政治倾向，大部分模型的政治画像是偏自由主义
![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030120137052-paste.png)
### 如何减轻偏见
- 偏见可能由训练数据导致
- 训练过程中的措施
- 产生答案的过程中改变
- 后处理，大模型输出答案之后，再加防御层（deepseek不生成政治相关问题）
### AIGC
![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030120455418-paste.png)
![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030120630984-paste.png)
- 让大模型在输出中加浮水印
### 大模型越狱和注入攻击方法
-
## 加速模型生成速度 Speculative Decoding
- 自回归模式使得大模型逐字生成，限制了大模型速度
-
![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030121915364-paste.png)
- 就算预言模型效果非常差，一个都猜不对，此时生成速度也只是回到了完全没有预言模块的状态
- 用空间换时间？？还是放到批次
- 可以用多个预言模型，来增加预测对的序列的几率（预测不对的就忽略）
- 预言模型
  - 非自回归模型，一次性生成多个 token
  - 蒸馏后的小模型
  - 搜索引擎
## 关于影像的生成式 AI
![](https://pic-gino-prod.oss-cn-qingdao.aliyuncs.com/zhangli2025/20251030123040693-paste.png)

## 大模型应用的三种境界
现在豆包、deepseek 模型有网页版本、手机 APP，普通用户使用起来非常方便。既然如此，为什么我们还要在豆包等大模型的基础上去开发，意义是什么，用户干嘛不去用豆包要来用我们开发的东西？？？
1. 赋能、润物细无声，大模型支撑着某项功能，但是用户完全感觉不到，大模型被包裹在功能内部
- AI 面试系统，以及评分
2. 利用大模型跟已有系统互动，用户可以感觉到大模型的存在。可代替强
- cursor 以及其他 IDE 中集成的 AI 聊天框
- CSDN 现在在编写文章的时候，右侧也有一个 AI 聊天框
- 牛客智搜，融入牛客特色的 AI 聊天框
3. 我们的东西完全和豆包等模型撞车，没有特色，没有区分度。属于意义不明，纯粹跟风，走个形式
- 我学校官网的 deepseek。deepseek 火出圈的时候，学校就在自己的网站上部署了一个（也可能是调用 API ），然后学院大群通知所有同学，我们学校有自己的 deepseek 了，欢迎体验
- 我在学校负责的“农标通大模型 AgristdLLM”
  - 它有自己独特的定位，面向农户，用于回答种植、养殖方面的问题，助力农业标准化生产
  - 它被精心调整过，采用了 “Lora 微调 + RAG”，光是收集数据就花了非常久，这一步进行的非常艰难
  - 它被部署在年租过万的服务器上面，其中注明了公测版，但现实是除了托以外 0 个人来测
  - 即使为了它付出了非常多心血，但是根本无人问津。究其原因，我认为是它可以做到的，豆包等模型也可以，用户会偏向于使用自己熟悉的模型，根本没必要打开一个陌生且卡的网站，如果这个网站没有一些仅自己有的，非常有特色的东西的话
  - 由于现在的大模型日益强大，农户的一些基本问题，通用大模型完全可以解决，开发农标通的意义在哪里。即使农标通开发的那么艰难，又是微调又是知识库，但它的性能没有明显优于最近出现的通用大模型，用户的体验可能是，我们调的是 API ，感觉上面的行动白费了
  - 导师原计划是好好宣称一下，让学院公众号写一篇推文，说我们组有自己的大模型了，现在已经有 xx 名用户注册，每个月的访问量高达 xx。但是泡汤了，推广非常难，很多人都懒得去注册
  - 反思：不识庐山真面目，只缘身在此山中。项目进行时自以为是第一个研究大模型的组，充满了自信，更多的是在向别的组展示，我们现在在研究这个
### 关于 API 一些高校的态度
- 我在学校的时候从来没有用过 API，看到了 open AI key 就会跳过这个项目，如果一个模型不开放权重（比如豆包），就会忽略掉它去找别的
- 如果在组会上跟老师说，我们可以用 API 肯定会被批评，会被反问：那你的工作体现在哪里
- 估计是因为，高校拿到课题经费是用来研究的，而 API 被封装好了，是别人的东西，没有一点研究价值。而且这种费用不好报销
- **UltraFlow 是否可以适用于本地部署的大模型，是否需要把本地大模型包装为接口形式，并符合 OpenAI 的 API 规范**
## 模型轻量化的趋势 和 基座大模型怎么选择？
## 还有一线生机？怎么拯救“农标通”？
- 选用轻量化模型代替 6B 的比较老的 chatGLM，节约成本
- 把重点放在该网站中独有的功能上，比如标准 pdf、服务提供者信息等
- 参照上面那篇生成专利的论文，能不能尝试让农标通生成标准规范文件的初稿，提供给编撰标准的学校师生用。
- **专注小众赛道**，避免侧重点和大厂通用模型重合
- 标准文件生成是一个很难的任务，生成标准文件本身就需要参考其他非常多的标准化文件（它们内部存在相互引用），就算是豆包也很难做好，而我们网站中正好存储了大量的标准文件，这可能是农标通死灰复燃的机会
- Todo: 跑一下专利生成的论文代码，评估一下可行性
-


#### 曾经看到的，非常震惊的论文《如何使用大模型生成一篇专利》
- 如果是生成短篇小说，那倒不奇怪，只要拆分任务，大模型完全可以做到（比如，无忌哥的大模型写书计划）
- 我曾经遇到过大模型算数算错了，我把 word 表格形式的成绩单（不存在 ORC 识别错数字的情况）复制到大模型中，让它算一下平均分，但是它计算错了，多算了 0.1 分？？奇怪为什么
- 大模型给人的印象是更擅长文本内容的生成，但是数字不擅长（比如，知乎曾经的热搜，0.001和0.18谁打，难道是因为大模型把数字当做文本处理，没有考虑到小数点后位数的特殊性吗）
- 专利不同于普通短文，它具有下面这些特点，所以看到可以直接生成一篇专利，当时真的非常震惊
  - 存在大量公式
  - 格式有严格的要求
  - 逻辑要非常严谨
  - 一篇专利非常的长
